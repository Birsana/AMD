# -*- coding: utf-8 -*-
"""AMDCopy.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1BrWXQxfKqoo2HQkZ8LXDxeDYQewbmPcH
"""

from __future__ import absolute_import, division, print_function, unicode_literals

import pathlib

import matplotlib.pyplot as plt
import pandas as pd
import seaborn as sns

import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
from keras import metrics

url = "https://raw.githubusercontent.com/Birsana/AMD/master/AMDINTBTCSP.csv"

df = pd.read_csv(url)
df = df.dropna()
df = df.drop(["Unnamed: 0"], axis = 1)
df = df.astype(float)

trainData = df.sample(frac=0.8, random_state=0)
testData = df.drop(trainData.index)

trainLabels = trainData.pop("Labels")
testLabels = testData.pop("Labels")

stats = trainData.describe()
stats = stats.transpose()


def norm(x):
    return (x - stats['mean']) / stats['std']


normTrainData = norm(trainData)
normTestData = norm(testData)

normTrainData["Bias"] = 1
normTestData["Bias"] = 1
print(normTrainData)

costs = []
whichModel = []

def buildModel(neuron, l, dropout):
    model = keras.Sequential([
    layers.Dense(neuron, activation=tf.nn.relu, input_shape=[len(normTrainData.keys())])
    ])
    for i in range(l):
        model.add(keras.layers.Dropout(dropout))
        model.add(layers.Dense(neuron, activation=tf.nn.relu))

    model.add(layers.Dense(2, activation=tf.nn.softmax))
    model.compile(optimizer='adam',
                  loss='sparse_categorical_crossentropy',
                  metrics=['accuracy'])
    return model


def multipleModels(neurons, l):
    for i in range(int(neurons/ 2), neurons):
        for drop in range(2,6):
            for j in range(l):
                model = buildModel(i, l, drop/10)
                getHistory(model)
                which = "%s, %s, %s" %(neurons, l, drop)
                whichModel.append(which)

class PrintDot(keras.callbacks.Callback):
    def on_epoch_end(self, epoch, logs):
        if epoch % 100 == 0: print('')
        print('.', end='')


EPOCHS = 1000
early_stop = keras.callbacks.EarlyStopping(monitor='val_loss', patience=3)


def getHistory(model):
    history = model.fit(
        normTrainData, trainLabels,
        epochs=EPOCHS, validation_split=0.1, verbose=0,
        callbacks=[early_stop, PrintDot()])

    hist = pd.DataFrame(history.history)
    hist['epoch'] = history.epoch
    hist.tail()
    costs.append(hist['val_loss'].iloc[-1])

multipleModels(10,3)

# testLoss, testAcc = model.evaluate(normTestData, testLabels)

# print('Test accuracy:', testAcc)
minInd = costs.index(min(costs))
bestNN = (whichModel[minInd])
print(bestNN)
